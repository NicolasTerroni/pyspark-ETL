{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381439ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using Ubuntu 20.04, Python 3.8.10, and Spark 3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a32a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import pyspark\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, TimestampType\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    concat,\n",
    "    col, \n",
    "    lit, \n",
    "    row_number, \n",
    "    monotonically_increasing_id, \n",
    "    to_date, \n",
    "    to_utc_timestamp, \n",
    "    date_format, \n",
    "    lpad,\n",
    "    from_unixtime,\n",
    "    unix_timestamp,\n",
    "    to_timestamp,\n",
    "    expr\n",
    ")\n",
    "\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3efbfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(f\"Spark version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0339801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/29 18:49:51 WARN Utils: Your hostname, nicolasterroni-asus-tuf resolves to a loopback address: 127.0.1.1; using 192.168.0.35 instead (on interface enp2s0)\n",
      "23/04/29 18:49:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/nicolasterroni/Snoop/snoop-etls/etl/venv/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/nicolasterroni/.ivy2/cache\n",
      "The jars for the packages stored in: /home/nicolasterroni/.ivy2/jars\n",
      "org.xerial#sqlite-jdbc added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6bf8984e-38c9-4552-97b7-110cf0d1343d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.xerial#sqlite-jdbc;3.40.0.0 in central\n",
      ":: resolution report :: resolve 111ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.xerial#sqlite-jdbc;3.40.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6bf8984e-38c9-4552-97b7-110cf0d1343d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/5ms)\n",
      "23/04/29 18:49:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Load CSV to SQLITE\") \\\n",
    "    .setMaster(\"local\") \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"/home/nicolasterroni/projects/spark/jars/*\") \\\n",
    "    .set(\"spark.jars.packages\", \"org.xerial:sqlite-jdbc:3.40.0.0\")\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00346436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your csv location\n",
    "main_path = '/home/nicolasterroni/projects/spark/examples/csv_cleaning/'\n",
    "\n",
    "# your csv filename\n",
    "csv_name = 'test.csv'\n",
    "\n",
    "# ensure that the delimiter is correct\n",
    "df = spark.read.format('csv').option('header', True).option('delimiter', '|').load(main_path+csv_name)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA MODELING\n",
    "\n",
    "# example of joining 2 dataframes\n",
    "# df = df.join(df2, on=\"id\", how=\"inner\")\n",
    "\n",
    "# delete column\n",
    "# df = df.drop(\"column\")\n",
    "\n",
    "# converting 'revenue' column from string like '55.000,00' to float\n",
    "#df = df.withColumn('revenue', col('revenue').replace('.', ''))\n",
    "#df = df.withColumn('revenue', col('revenue').replace(',', '.'))\n",
    "#df = df.withColumn('revenue', col('revenue').cast('float'))\n",
    "\n",
    "# check the transformed data\n",
    "# df.printSchema()\n",
    "# df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c42d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING\n",
    "\n",
    "# examples of changing dates and datetimes format\n",
    "#df = df.withColumn(\"date\", from_unixtime(unix_timestamp(df[\"date\"], \"d/M/yyyy\"), \"yyyy-MM-dd\"))\n",
    "#df = df.withColumn(\"datetime\", from_unixtime(unix_timestamp(df[\"datetime\"], \"d/M/yyyy HH:mm:ss\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# example of string manipulation, geting a substring\n",
    "#df = df.withColumn(\"string\", expr(\"substring(string, 1, length(string) - 4)\"))\n",
    "\n",
    "# convert string to timestamp\n",
    "#df = df.withColumn(\"datetime\", to_timestamp(df[\"datetime\"], \"yyyy/MM/dd HH:mm:ss\"))\n",
    "\n",
    "# check the transformed data\n",
    "# df.printSchema()\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the schema, to create table if it doesnt exist\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "])\n",
    "\n",
    "\n",
    "url = 'jdbc:sqlite:/your/sqlite/location/test.db'\n",
    "table_name = 'test'\n",
    "\n",
    "mode = 'overwrite' # TRUNCATE\n",
    "\n",
    "properties = {\n",
    "    'driver': 'org.sqlite.JDBC',\n",
    "}\n",
    "\n",
    "conn = sqlite3.connect('test.db')\n",
    "\n",
    "print(\"Connected to sqlite db\")\n",
    "\n",
    "\n",
    "# create table if it does not exist\n",
    "conn.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        name TEXT,\n",
    "        age TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Database created\")\n",
    "\n",
    "# write\n",
    "df.write.mode('overwrite').option('driver', 'org.sqlite.JDBC').jdbc(url=url, table=table_name, mode=mode)\n",
    "print(f\"Written data: {table_name} in database.\")\n",
    "\n",
    "# check the written data\n",
    "read_df = spark.read.format('jdbc').option('driver', 'org.sqlite.JDBC').option('url', url).option('dbtable', table_name).load()\n",
    "print(\"Actual data from database:\")\n",
    "read_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
