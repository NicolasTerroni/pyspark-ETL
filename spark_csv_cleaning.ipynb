{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381439ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using Ubuntu 20.04, Python 3.8.10, and Spark 3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469da0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyspark\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    concat,\n",
    "    col, \n",
    "    lit, \n",
    "    row_number, \n",
    "    monotonically_increasing_id, \n",
    "    to_date, \n",
    "    to_utc_timestamp, \n",
    "    date_format, \n",
    "    lpad,\n",
    "    from_unixtime,\n",
    "    unix_timestamp,\n",
    "    to_timestamp,\n",
    "    expr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3efbfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(f\"Spark version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"CSV Cleaning\") \\\n",
    "    .setMaster(\"local\") \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"/home/nicolasterroni/projects/spark/jars/*\")\n",
    "# you must specify the jars location\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4711b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your csv location\n",
    "main_path = '/home/nicolasterroni/projects/spark/examples/csv_cleaning/'\n",
    "\n",
    "# your csv filename\n",
    "csv_name = 'test.csv'\n",
    "\n",
    "# ensure that the delimiter is correct\n",
    "df = spark.read.format('csv').option('header', True).option('delimiter', '|').load(main_path+csv_name)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc464f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the initial data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "923ecf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA MODELING\n",
    "\n",
    "# example of joining 2 dataframes\n",
    "# df = df.join(df2, on=\"id\", how=\"inner\")\n",
    "\n",
    "# delete column\n",
    "# df = df.drop(\"column\")\n",
    "\n",
    "# converting 'revenue' column from string like '55.000,00' to float\n",
    "#df = df.withColumn('revenue', col('revenue').replace('.', ''))\n",
    "#df = df.withColumn('revenue', col('revenue').replace(',', '.'))\n",
    "#df = df.withColumn('revenue', col('revenue').cast('float'))\n",
    "\n",
    "# check the transformed data\n",
    "# df.printSchema()\n",
    "# df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING\n",
    "\n",
    "# examples of changing dates and datetimes format\n",
    "#df = df.withColumn(\"date\", from_unixtime(unix_timestamp(df[\"date\"], \"d/M/yyyy\"), \"yyyy-MM-dd\"))\n",
    "#df = df.withColumn(\"datetime\", from_unixtime(unix_timestamp(df[\"datetime\"], \"d/M/yyyy HH:mm:ss\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# example of string manipulation, geting a substring\n",
    "#df = df.withColumn(\"string\", expr(\"substring(string, 1, length(string) - 4)\"))\n",
    "\n",
    "# convert string to timestamp\n",
    "#df = df.withColumn(\"datetime\", to_timestamp(df[\"datetime\"], \"yyyy/MM/dd HH:mm:ss\"))\n",
    "\n",
    "# check the transformed data\n",
    "# df.printSchema()\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a3d1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process took 353.34016513824463 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Spark's parallel processing writes each fragment of the csv by separated, we need to join them\n",
    "# I added the 'clean_' prefix to recognize the cleaned csv from the original\n",
    "\n",
    "# set a temporary save path\n",
    "save_path = main_path + \"/tmp/\" + csv_name[:-4]\n",
    "\n",
    "# ensure to specify your wanted delimiter\n",
    "df.coalesce(1).write.options(header='True', delimiter='|').mode(\"overwrite\").csv(save_path)\n",
    "\n",
    "for file in os.listdir(save_path):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        os.remove(save_path+'/'+file)\n",
    "    else:\n",
    "        os.rename(save_path+'/'+file, main_path+'/'+\"clean_\"+csv_name)\n",
    "\n",
    "shutil.rmtree(main_path + \"/tmp\")\n",
    "spark.stop()\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"The process took {total_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.5gb CSV CLEANED AND RE-WRITTEN IN -----------> 353.3 SECONDS\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
